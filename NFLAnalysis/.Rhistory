if(length(nodes) > 0 ) {
measures <- sapply(nodes, xmlValue)
#Clean up the column name
measures <- gsub(” *[0-9]*:”, “”, gsub(” \\(.*?\\)[0-9]*:”,””, measures))
#Remove dups
dups <- which(duplicated(measures))
#print(dups)
for(i in 1:length(dups))
measures[dups[i]] = paste(measures[dups[i]], i, sep=” “)
#use siblings function to get value
values <- sapply(nodes, function(x)  xmlValue(getSibling(x)))
install.packages("Quandl")
library(Quandl)
symbol = "HD"
url <- paste('https://finance.yahoo.com/quote/xom/key-statistics?p=xom',symbol,sep="")
webpage <- readLines(url)
html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
tableNodes <- getNodeSet(html, "//table")
View(tableNodes)
View(what_metrics)
#Estimates
symbol = "HD"
url <- paste('https://finance.yahoo.com/quote/HD/analysts?p=',symbol,sep="")
webpage <- readLines(url)
html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
tableNodes <- getNodeSet(html, "//table")
View(tableNodes)
symbol = "HD"
url <- paste('https://finance.yahoo.com/quote/xom/key-statistics?p=xom',symbol,sep="")
webpage <- readLines(url)
html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
tableNodes <- getNodeSet(html, "//table")
HD.EV <- readHTMLTable(tableNodes[[1]])
View(HD.EV)
library("XML")
library("plyr")
#Estimates
symbol1 = "HD"
HD.url <- paste('https://finance.yahoo.com/quote/HD/analysts?p=',symbol,sep="")
HD.webpage <- readLines(url)
HD.html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
HD.tableNodes <- getNodeSet(html, "//table")
HD.EE <- readHTMLTable(tableNodes[[1]])
HD.RE <- readHTMLTable(tableNodes[[2]])
#Estimates
symbol1 = "HD"
HD.url <- paste('https://finance.yahoo.com/quote/HD/analysts?p=',symbol,sep="")
HD.webpage <- readLines(url)
HD.html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
HD.tableNodes <- getNodeSet(html, "//table")
symbol1 = "HD"
HD.url <- paste('https://finance.yahoo.com/quote/HD/analysts?p=',symbol,sep="")
symbol = "HD"
HD.url <- paste('https://finance.yahoo.com/quote/HD/analysts?p=',symbol,sep="")
HD.webpage <- readLines(url)
HD.html <- htmlTreeParse(webpage, useInternalNodes = TRUE, asText = TRUE)
HD.tableNodes <- getNodeSet(html, "//table")
#Estimates
symbol1 = "HD"
HD.url <- paste('https://finance.yahoo.com/quote/HD/analysts?p=',symbol1,sep="")
HD.webpage <- readLines(url)
#Estimates
symbol1 = "HD"
HD.url <- paste('https://finance.yahoo.com/quote/HD/analysts?p=',symbol1,sep="")
HD.webpage <- readLines(HD.url)
#Estimates
symbol1 = "HD"
HD.url <- paste('https://finance.yahoo.com/quote/HD/analysts?p=',symbol1,sep="")
HD.webpage <- readLines(HD.url)
HD.html <- htmlTreeParse(HD.webpage, useInternalNodes = TRUE, asText = TRUE)
HD.tableNodes <- getNodeSet(HD.html, "//table")
View(HD.tableNodes)
HD.EE <- readHTMLTable(tableNodes[[1]])
HD.RE <- readHTMLTable(tableNodes[[2]])
HD.EE <- readHTMLTable(HD.tableNodes[[1]])
HD.RE <- readHTMLTable(HD.tableNodes[[2]])
View(HD.EE)
View(HD.RE)
#Key Stats
HD.KS.url <- paste('https://finance.yahoo.com/quote/xom/key-statistics?p=xom',symbol,sep="")
HD.KS.webpage <- readLines(HD.KS.url)
HD.KS.html <- htmlTreeParse(HD.KS.webpage, useInternalNodes = TRUE, asText = TRUE)
HD.KS.tableNodes <- getNodeSet(HD.KS.html, "//table")
HD.EV <- readHTMLTable(HD.KS.tableNodes[[1]])
View(HD.EV)
summary(HD.RE)
View(HD.RE)
HD.RE[2,3]
HD.RE[2,4]
HD.KS <- readHTMLTable(HD.KS.tableNodes[[1]])
summary(HD.KS)
View(HD.KS)
HD.EV <- HD.KS[2,2]
HD.EV <- HD.KS[2,2]
HD.KS <- readHTMLTable(HD.KS.tableNodes[[1]])
HD.EV <- HD.KS[2,2]
HD.EV
View(HD.KS)
HD.EVtoS <- HD.RE / HD
HD.EVtoS <- HD.EV / HD.FY19
HD.FY19 <- HD.RE[2,4]
HD.EV <- HD.KS[2,2]
HD.EVtoS <- (HD.EV) / (HD.FY19)
factor(HD.FY19)
factor(HD.EV)
HD.EVtoS <- (HD.EV) / (HD.FY19)
HD.EVtoS <- HD.EV / HD.FY19
HD.FY19 <- as.numeric(as.character( HD.RE[2, 4] ))
HD.FY19 <- as.numeric( HD.RE[2, 4] )
HD.FY19 <- as.numeric(as.character(HD.RE[2,2]))
gsub("C([0-9]+)_.*", "\\1", HD.RE$Col2)
gsub("C([0-9]+)_.*", "\\1", HD.RE$Col2)
View(HD.RE)
gsub("C([0-9]+)_.*", "\\1", HD.RE$Current Year (2019))
gsub("C([0-9]+)_.*", "\\1", HD.RE$CurrentYear(2019))
> gsub("[a-zA-Z ]", "", HD.RE)
gsub("[a-zA-Z ]", "", HD.RE)
View(HD.RE)
HD.RE2 <- gsub("[a-zA-Z ]", "", HD.RE)
library("ggplot2")
library("psych")
#chisq
chisq.test(sn.tab)
#create crosstabs for cat vars
sn <- read.csv("social_network.csv", header = T)
#create contingency table
sn.tab <- table(sn$Gender, sn$Site)
#chisq
chisq.test(sn.tab)
#comparing means with t-test
names(google)
#creating scatterplots
google <- read.csv("google_correlate.csv", header = T)
#comparing means with t-test
names(google)
t.test(google$nba ~ google$has_nba)
#ANOVA Test
google <- read.csv(google_correlate.csv, header = T)
#ANOVA Test
google <- read.csv("google_correlate.csv, header = T)
#Oneway ANOVA
anova1 <- aov(data_viz ~ region, data = google)
summary(anova1)
summary(anova1)
anova1 <- aov(data_viz ~ region, data = google)
summary(anova1)
summary(anova1)
#creating scatterplots
google <- read.csv("google_correlate.csv", header = T)
anova1 <- aov(data_viz ~ region, data = google)
summary(anova1)
x <- c(1,2,3)
y <- c(2,4,6)
z <- c("male","female","male")
x+2
x+y
matrix(c(x,z),nrow=3,ncol=2)
matrix(c(x,factor(z)),nrow=3,ncol=2)
data.frame(x,z)
install.packages(fivethirtyeight)
install.packages()
packages()
packages
.packages()
installed.packages("fivethirtyeight")
install.packages("fivethirtyeight")
library(fivethirtyeight)
#2a
bad_drivers[1:5]
#2a
bad_drivers[1:5,]
#2a
bad_drivers[1:5,]
#2a
bad_drivers[1:5,]
bad_drivers
#2a
firstfive <- bad_drivers[1:5,]
View(firstfive)
#2b
help.search("perc_not_distracted")
#2b
help.search("perc_not_distracted")
#2b
help.search()
#2b
help.search(dataset)
#2b
RSiteSearch("perc_not_distracted")
x+2
x+y
#1c
matrix(c(x,z),nrow=3,ncol=2)
matrix(c(x,factor(z)),nrow=3,ncol=2)
data.frame(x,z)
#2a
head(bad_drivers, n=5)
#2a
firstfive <- bad_drivers[1:5,]
View(firstfive)
install.packages("openintro")
packages("openintro")
library("openintro")
library(openintro)
cherry
a <- c(1,7,5,10,8)
mat1 <- (1:12, nrow=4, ncol=3)
mat1 <- matrix(1:12, nrow=4, ncol=3)
mat1
mat1 <- matrix(1:12, nrow=4, ncol=3, byrow=TRUE)
mat1
mat2 <- matrix(1:12, nrow=4, ncol=3, byrow=TRUE)
mat2 <- matrix(1:12, nrow=4, ncol=3, byrow=FALSE)
mat2
mat2 <- matrix(1:12, nrow=4, ncol=3)
mat2
1stsix <- c(1:6)
firstsix <- c(1:6)
colors <- c("green","green","red","blue","yellow","red")
first <- c(1,2,3,4,5,6)
second <- c(4,7,2,8,2,10)
datfr <- data.frame(colors,first,second)
View(datfr)
objs <- list(a,mat1,mat2,datfr)
View(objs)
X <- c(2, 7, 6)
Y <- c(3, 2, 5)
X + Y
cherry
mat1 <- matrix(1:12, nrow=4, ncol=3, byrow=TRUE)
mat1
mat2
a <- c(1,7,5,10,8)
a
mat1
mat2
datfr <- data.frame(colors,first,second)
View(datfr)
View(objs)
objs
objs
X + Y
grades <- read.table("C:/Users/Ramiz/Desktop/SASUniversityEdition/myfolders/Grades.txt",
sep = "&", header = TRUE)
setwd("C:/Users/Ramiz/Desktop/SASUniversityEdition/myfolders")
grades <- read.table("C:/Users/Ramiz/Documents/RProjects/Grades.txt",
sep = "&", header = TRUE)
setwd("C:/Users/Ramiz/Documents/RProjects")
setwd("~/")
grades <- read.table("Grades.txt",
sep = "&", header = TRUE)
grades <- read.table("Grades.txt",
sep = "&", header = TRUE)
library(taRifx)
library(tidyverse)
library(dplyr)
library(fuzzyjoin)
library(plotly)
#Set working directory
setwd("~/Documents/GitHub/RProjects/NFLAnalysis")
#Read in 2018 regular passing data from Pro Football Reference and rename select columns
passing_stats <- read.csv(file = "./2018_reg_passing_stats.csv", header = TRUE, sep = ",") %>%
rename(Name = Player, Team = Tm, CmpPct = Cmp.)
#Read in 2018 advanced passing data from Pro Football Reference and rename select columns
adv_passing_stats <- read.csv(file = "./2018_adv_passing_stats.csv", header = TRUE, sep = ",") %>%
rename(DropPct = Drop., BadPct = Bad., Name = Player, Team = Tm)
#Read in initial Madden 20 player ratings CSV
madden_ratings <- read.csv(file = "./M20PlayerRatingsJul19.csv", header = TRUE, sep = ",") %>%
rename(Pos = position)
#Clean up col names in ratings df
cleancols <- sub(".rating", "", names(madden_ratings), fixed = TRUE)
names(madden_ratings) <- cleancols
#Remove all non-alphanumeric characters
madden_ratings$Name <- str_replace_all(madden_ratings$Name, "[^A-Za-z ]", "")
passing_stats$Name <- str_replace_all(passing_stats$Name, "[^A-Za-z ]", "")
adv_passing_stats$Name <- str_replace_all(adv_passing_stats$Name, "[^A-Za-z ]", "")
#Join regular passing and advanced passing data into one dataframe
all_passing_stats <- inner_join(passing_stats, adv_passing_stats)
#Since there seems to be passing data for non QBs, let's filter for the QB position only
all_passing_stats_filt <- all_passing_stats %>% filter(grepl("qb", Pos, ignore.case = TRUE))
ratings_filt <- madden_ratings %>% filter(grepl("qb", Pos, ignore.case = TRUE))
head(madden_ratings)
View(all_passing_stats_filt)
head(ratings_filt)
glimpse(ratings_filt)
#First take a peek at what we have available in the Madden ratings dataset
glimpse(ratings_filt)
madden_ratings_select <- ratings_filt %>%
select(Name, Team, Pos, ovr, awareness, throw.power, strength, play.action, acceleration, injury,
throw.accuracy.short, throw.accuracy.deep, break.sack, toughness, agility, elusiveness)
glimpse(madden_ratings_select)
#Read in 2018 regular passing data from Pro Football Reference and rename select columns
passing_stats <- read.csv(file = "./2018_reg_passing_stats.csv", header = TRUE, sep = ",") %>%
rename(Name = Player, Team = Tm, CmpPct = Cmp.)
#Read in 2018 advanced passing data from Pro Football Reference and rename select columns
adv_passing_stats <- read.csv(file = "./2018_adv_passing_stats.csv", header = TRUE, sep = ",") %>%
rename(DropPct = Drop., BadPct = Bad., Name = Player, Team = Tm)
#Read in initial Madden 20 player ratings CSV
madden_ratings <- read.csv(file = "./M20PlayerRatingsJul19.csv", header = TRUE, sep = ",") %>%
rename(Pos = position)
#Clean up col names in ratings df
cleancols <- sub(".rating", "", names(madden_ratings), fixed = TRUE)
names(madden_ratings) <- cleancols
#Remove all non-alphanumeric characters
madden_ratings$Name <- str_replace_all(madden_ratings$Name, "[^A-Za-z ]", "")
passing_stats$Name <- str_replace_all(passing_stats$Name, "[^A-Za-z ]", "")
adv_passing_stats$Name <- str_replace_all(adv_passing_stats$Name, "[^A-Za-z ]", "")
#Join regular passing and advanced passing data into one dataframe
all_passing_stats <- inner_join(passing_stats, adv_passing_stats)
#Since there seems to be passing data for non QBs, let's filter for the QB position only
all_passing_stats_filt <- all_passing_stats %>% filter(grepl("qb", Pos, ignore.case = TRUE))
madden_ratings_filt <- madden_ratings %>% filter(grepl("qb", Pos, ignore.case = TRUE))
madden_ratings_filt_select <- madden_ratings_filt %>%
select(Name, Team, Pos, ovr, awareness, throw.power, strength, play.action, acceleration, injury,
throw.accuracy.short, throw.accuracy.deep, break.sack, toughness, agility, elusiveness)
#First take a peek at what we have available in tthe all passing dataframe
glipmse(all_passing_stats_filt)
#First take a peek at what we have available in tthe all passing dataframe
glimpse(all_passing_stats_filt)
#Seems like it will be easier if we drop the columns we don't need, and let's also rename some of the columns for ease of use later (dictionary available at Pro Football Reference)
all_passing_stats_filt_select <- all_passing_stats_filt %>% select(-c(GS, QBrec, TD., Int.)) %>%
rename(FirstD=X1D, SkYds = Yds.1, SkPct = Sk., Q4CBK = X4QC)
#Add a few calculations for QBs
all_passing_stats_filt_select <- all_passing_stats_filt_select %>%
mutate(TDtoInt = TD/Int)
glimpse(all_passing_stats_filt_select)
#Add a few calculations for QBs
all_passing_stats_filt_select <- all_passing_stats_filt_select %>%
mutate(TDtoInt = round(TD/Int, 2))
glimpse(all_passing_stats_filt_select)
#Add a few calculations for QBs
all_passing_stats_filt_select <- all_passing_stats_filt_select %>%
mutate(TDtoInt = round(TD/Int, 2), #Nothing fancy, just a simple ratio to how often a QB throws an errant pass (or can we blane WRs?)
SktoBltz = Sk/Bltz)
joined_data <- stringdist_left_join(all_passing_stats_filt_select, madden_ratings_filt_select)
View(joined_data)
joined_data <- stringdist_inner_join(all_passing_stats_filt_select, madden_ratings_filt_select)
View(joined_data)
glimpse(joined_data)
#Inner join the ratings data to stats for 2019 QBs. Drop the duplicate col and clean col names
joined_data <- stringdist_inner_join(all_passing_stats_filt_select, madden_ratings_filt_select) %>%
select(-c(Name.y,Team.y,Pos.y)) %>% rename(Name = Name.x, Team = Team.x, Pos = Pos.x)
#Hmm, seems like we dropped 16 players in the join. Let's see what happened.
madden_ratings_filt_select %>% filter(Name %!in% joined_data$Name)
#Create a "not in" operator.  I use this heavily in all of my analyses and begin every script with this.
`%!in%` = Negate(`%in%`)
#Hmm, seems like we dropped 16 players in the join. Let's see what happened.
madden_ratings_filt_select %>% filter(Name %!in% joined_data$Name)
#Hmm, seems like we dropped 16 players in the join. Let's see what happened.
madden_ratings_filt_select %>% filter(Name %!in% joined_data$Name) %>% select(Name)
#Inner join the ratings data to stats for 2019 QBs. Drop the duplicate col and clean col names
joined_data <- stringdist_left_join(all_passing_stats_filt_select, madden_ratings_filt_select) %>%
select(-c(Name.y,Team.y,Pos.y)) %>% rename(Name = Name.x, Team = Team.x, Pos = Pos.x)
#If there is no ovr rating, must mean the Madden dataset didn't have a value
not_in_madden <- joined_data %>% filter(is.na(ovr))
View(not_in_madden)
#Lets see if they exist in the raw datasets
madden_ratings %>% filter(grepl("carr", Name, ignore.case = TRUE))
#Lets see if they exist in the raw datasets
madden_ratings %>% filter(grepl(c("Carr","Foles"), Name, ignore.case = TRUE))
#Create a string of names to search for
name_search <- c("carr", "foles", "tannehill")
madden_ratings %>% filter(grepl(name_search, Name, ignore.case = TRUE))
paste0(name_search)
paste0(name_search, collapse = "|")
madden_ratings %>% filter(grepl(paste0(name_search, collapse = "|"), Name, ignore.case = TRUE))
joined_data <- stringdist_left_join(all_passing_stats_filt_select, madden_ratings_filt_select, by = "Name")
#If there is no ovr rating, must mean the Madden dataset didn't have a value
not_in_madden <- joined_data %>% filter(is.na(ovr))
View(not_in_madden)
#Create a string of names to search for
name_search <- c("carr", "foles", "tannehill", "Osweiler")
madden_ratings %>% filter(grepl(paste0(name_search, collapse = "|"), Name, ignore.case = TRUE))
joined_data <- stringdist_left_join(all_passing_stats_filt_select, madden_ratings_filt_select, by = "Name") %>%
select(-c(Name.y,Team.y,Pos.y)) %>% rename(Name = Name.x, Team = Team.x, Pos = Pos.x)
View(joined_data)
joined_data <- stringdist_left_join(all_passing_stats_filt_select, madden_ratings_filt_select) %>%
select(-c(Name.y,Team.y,Pos.y)) %>% rename(Name = Name.x, Team = Team.x, Pos = Pos.x)
#Let's see if we have both types (Madden rating and passing data for all players)
#If there is no ovr rating, must mean the Madden dataset didn't have a value
not_in_madden <- joined_data %>% filter(is.na(ovr))
#Now let's recheck to see if we have any missing rating data
not_in_madden <- joined_data %>% filter(is.na(ovr))
#Inner join the ratings data to stats for 2019 QBs. Drop the duplicate col and clean col names
joined_data <- stringdist_left_join(all_passing_stats_filt_select, madden_ratings_filt_select) %>%
select(-c(Name.y,Team.y,Pos.y)) %>% rename(Name = Name.x, Team = Team.x, Pos = Pos.x)
#Let's see if we have both types (Madden rating and passing data for all players)
#If there is no ovr rating, must mean the Madden dataset didn't have a value
not_in_madden <- joined_data %>% filter(is.na(ovr))
#Now let's recheck to see if we have any missing rating data
not_in_madden <- joined_data %>% filter(is.na(ovr))
#Redo the join, this time explictly with a "by" argument
joined_data <- stringdist_left_join(all_passing_stats_filt_select, madden_ratings_filt_select, by = "Name") %>%
select(-c(Name.y,Team.y,Pos.y)) %>% rename(Name = Name.x, Team = Team.x, Pos = Pos.x)
#Now let's recheck to see if we have any missing rating data
not_in_madden <- joined_data %>% filter(is.na(ovr))
View(not_in_madden)
#Create a string of names to search for
name_search <- c("osweiler","sanchez")
madden_ratings %>% filter(grepl(paste0(name_search, collapse = "|"), Name, ignore.case = TRUE))
#Generate correleation matrix
statscorr <- data.frame(cor(joined_data[,unlist(lapply(joined_data, is.numeric))]))
View(statscorr)
passing_filt <- japply(passing_filt, which(sapply(passing_filt, class)=="integer"), as.numeric )
install.packages("ggcorrplot")
library(ggcorrplot)
ggcorrplot(statscorr)
ggcorrplot(statscorr, method = "circle")
View(statscorr)
View(joined_data)
#Drop the 6 players where we know we don't have ratings for them (using ovr variable)
joined_data_filt <- joined_data %>% filter(!is.na(ovr))
#Generate correleation matrix
statscorr <- data.frame(cor(joined_data_filt[,unlist(lapply(joined_data_filt, is.numeric))]))
#Visualize matrix with ggcorrplot
ggcorrplot(statscorr) #Yikes, this is alot to process. Pretty much unusable.
ggcorrplot(statscorr, method = "circle")
ggcorrplot(statscorr, type = "lower")
#Seems like it will be easier if we drop the columns we don't need, and let's also rename some of the columns for ease of use later (dictionary available at Pro Football Reference)
all_passing_stats_filt_select <- all_passing_stats_filt %>% select(-c(Rk, GS, QBrec, TD., Int.)) %>%
rename(FirstD=X1D, SkYds = Yds.1, SkPct = Sk., Q4CBK = X4QC)
#Add a few calculations for QBs
all_passing_stats_filt_select <- all_passing_stats_filt_select %>%
mutate(TDtoInt = round(TD/Int, 2), #Nothing fancy, just a simple ratio to how often a QB throws an errant pass (or can we blane WRs?)
SktoBltz = Sk/Bltz)
#Inner join the ratings data to stats for 2019 QBs. Drop the duplicate col and clean col names
joined_data <- stringdist_left_join(all_passing_stats_filt_select, madden_ratings_filt_select) %>%
select(-c(Name.y,Team.y,Pos.y)) %>% rename(Name = Name.x, Team = Team.x, Pos = Pos.x)
#Let's see if we have both types (Madden rating and passing data for all players)
#If there is no ovr rating, must mean the Madden dataset didn't have a value
not_in_madden <- joined_data %>% filter(is.na(ovr))
#Some of these make sense like Sam Bradford and Mark Sanchez (left the league), but what about others like Derek Carr and Nick Foles?
#Lets see if they exist in the raw datasets
#Create a string of names to search for
name_search <- c("carr", "foles", "tannehill")
madden_ratings %>% filter(grepl(paste0(name_search, collapse = "|"), Name, ignore.case = TRUE))
#So it seems like the players exist, but I didn't specify a join by above so R defaulted to joining by Name, Pos, and Team (which is different as players have either been traded/signed elsewhere)
#Redo the join, this time explictly with a "by" argument
joined_data <- stringdist_left_join(all_passing_stats_filt_select, madden_ratings_filt_select, by = "Name") %>%
select(-c(Name.y,Team.y,Pos.y)) %>% rename(Name = Name.x, Team = Team.x, Pos = Pos.x)
#Now let's recheck to see if we have any missing rating data
not_in_madden <- joined_data %>% filter(is.na(ovr))
#Looks much better - with only 6 players unmatched. Double check to make sure.
#Create a string of names to search for
name_search <- c("osweiler","sanchez")
madden_ratings %>% filter(grepl(paste0(name_search, collapse = "|"), Name, ignore.case = TRUE))
#Wrong Sanchez, but that's a good thing. Let's keep moving.
#Drop the 6 players where we know we don't have ratings for them (using ovr variable)
joined_data_filt <- joined_data %>% filter(!is.na(ovr))
### Start comparing ratings and looking at the data ###
#Generate correleation matrix
statscorr <- data.frame(cor(joined_data_filt[,unlist(lapply(joined_data_filt, is.numeric))]))
ggcorrplot(statscorr, type = "lower")
ggcorrplot(statscorr, type = "upper")
ageinj <- joined_data_filt %>%
ggplot(mapping = aes(x=Age, y=injury, group = 1,
text = paste("Name: ", Name, "<br>Age: ", Age, "<br>Injury: ", injury))) +
geom_point() +
geom_smooth(method='lm')
ggplotly(ageinj, tooltip = "text")
lm(joined_data_filt$injury ~ joined_data_filt$Age)
injury_age_lm <- lm(injury ~ age, data=joined_data_filt)
injury_age_lm <- lm(injury ~ Age, data=joined_data_filt)
summary(injury_age_lm)
ageinj <- joined_data_current %>%
ggplot(mapping = aes(x=Hits, y=injury, group = 1,
text = paste("Name: ", Name, "<br>Hits: ", Hits, "<br>Injury: ", injury))) +
geom_point() +
geom_smooth(method='lm')
ggplotly(ageinj, tooltip = "text")
ageinj <- joined_data_filt %>%
ggplot(mapping = aes(x=Hits, y=injury, group = 1,
text = paste("Name: ", Name, "<br>Hits: ", Hits, "<br>Injury: ", injury))) +
geom_point() +
geom_smooth(method='lm')
ggplotly(ageinj, tooltip = "text")
ageinj <- joined_data_filt %>%
ggplot(mapping = aes(x=Age, y=injury, group = 1,
text = paste("Name: ", Name, "<br>Age: ", Age, "<br>Injury: ", injury))) +
geom_point() +
geom_smooth(method='lm')
ggplotly(ageinj, tooltip = "text")
hitsinj <- joined_data_filt %>%
ggplot(mapping = aes(x=Hits, y=injury, group = 1,
text = paste("Name: ", Name, "<br>Hits: ", Hits, "<br>Injury: ", injury))) +
geom_point() +
geom_smooth(method='lm')
ggplotly(ageinj, tooltip = "text")
hitsinj <- joined_data_filt %>%
ggplot(mapping = aes(x=Hits, y=injury, group = 1,
text = paste("Name: ", Name, "<br>Hits: ", Hits, "<br>Injury: ", injury))) +
geom_point() +
geom_smooth(method='lm')
ggplotly(hitsinj, tooltip = "text")
hrrybadth <- joined_data_current %>%
ggplot(mapping = aes(x=Hrry, y=BadPct, group = 1,
text = paste("Name: ", Name, "<br>Hurry: ", Hrry, "<br>BadPct: ", BadPct))) +
geom_point() +
geom_smooth(method='lm')
ggplotly(hrrybadth, tooltip = "text")
hrrybadth <- joined_data_filt %>%
ggplot(mapping = aes(x=Hrry, y=BadPct, group = 1,
text = paste("Name: ", Name, "<br>Hurry: ", Hrry, "<br>BadPct: ", BadPct))) +
geom_point() +
geom_smooth(method='lm')
ggplotly(hrrybadth, tooltip = "text")
hrrybadth <- joined_data_filt %>%
ggplot(mapping = aes(x=Hrry, y=BadTh, group = 1,
text = paste("Name: ", Name, "<br>Hurry: ", Hrry, "<br>BadPct: ", BadPct))) +
geom_point() +
geom_smooth(method='lm')
ggplotly(hrrybadth, tooltip = "text")
